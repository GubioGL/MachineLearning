{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import linear_model\n",
    "from torch import nn\n",
    "from numpy.random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import torch  as tc\n",
    "\n",
    "class SineActivation(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(SineActivation, self).__init__() \n",
    "    def forward(self, x):\n",
    "        return tc.sin(x)\n",
    "    \n",
    "class Regressao_M(nn.Module):\n",
    "    def __init__(self, neuronio, M, output=3, activation=nn.Mish(), creat_p=False, N_of_paramater=1):\n",
    "        super().__init__()\n",
    "        self.neuronio = neuronio\n",
    "        self.M = M\n",
    "        self.output = output\n",
    "        self.creat_p = creat_p\n",
    "        self.N_of_paramater = N_of_paramater\n",
    "\n",
    "        # Lista para armazenar as camadas lineares\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(3, neuronio)])\n",
    "        self.hidden_layers.extend([nn.Linear(neuronio, neuronio) for _ in range(M-1)])\n",
    "        \n",
    "        # Última camada linear\n",
    "        self.output_layer = nn.Linear(neuronio, output)\n",
    "\n",
    "        # Função de ativação\n",
    "        self.activation = activation\n",
    "\n",
    "        if creat_p:\n",
    "            self.acceleration = nn.Parameter(tc.rand(N_of_paramater))\n",
    "            #self.acceleration = nn.Parameter(tc.ones(N_of_paramater)*2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.activation(layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "#generation of data\n",
    "def numerical_solution2(N,G0,c1,c2,c3,c4,tau,w,V0):\n",
    "    def f(t,q,V):\n",
    "        r=-q/tau+c1*V+c2*V**2+c3*V**3+c4*V**4\n",
    "        return r\n",
    "    def v(t,w):\n",
    "        return V0*np.cos(w*t)\n",
    "    Nc = 20\n",
    "    dt = 4*np.pi/(N)/w\n",
    "\n",
    "    n  = np.zeros(N)\n",
    "    I  = np.zeros(N)\n",
    "    V  = np.zeros(N)\n",
    "    t  = np.zeros(N)\n",
    "    n[0] = 6\n",
    "    I[0] = 1\n",
    "    t[0] = 0\n",
    "    V[0] = v(t[0],w)\n",
    "    for j in range(Nc):\n",
    "        if j>1:\n",
    "            n[0]=n[-1]\n",
    "            I[0]=I[-1]\n",
    "        for i in range(N-1):\n",
    "        #RK4\n",
    "            t[i+1]=t[i]+dt\n",
    "            k1=dt*f(t[i],n[i],v(t[i],w));\n",
    "            k2=dt*f(t[i]+dt/2,n[i]+k1/2,v(t[i]+dt/2,w));\n",
    "            k3=dt*f(t[i]+dt/2,n[i]+k2/2,v(t[i]+dt/2,w));\n",
    "            k4=dt*f(t[i]+dt,n[i]+k3,v(t[i]+dt,w));\n",
    "            n[i+1]=n[i]+(1.0/6.0)*(k1+k4+2.0*(k2+k3))\n",
    "            I[i+1]=(G0+n[i+1])*v(t[i+1],w)\n",
    "            V[i+1]=v(t[i+1],w)\n",
    "    return t,I,V,n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDO solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treino(lr_,neuronio_,camadas,fun_activation,epochs=1000,step_size_=500,N=1000,G0=1,c1=1.0,c2=1,c3=0,c4=0,tau=1,w=1,V0=1):\n",
    "    # Criando os dados de treino\n",
    "    time,corrent,d_potencial,x_true = numerical_solution2(N,G0,c1,c2,c3,c4,tau,w,V0)\n",
    "    I_n,d_pot = corrent/corrent.max(),d_potencial/d_potencial.max()\n",
    "\n",
    "    t_train = tc.tensor(time   ,requires_grad=True,dtype=tc.float).reshape(-1,1)\n",
    "    I_data  = tc.tensor(I_n    ,dtype=tc.float).reshape(-1,1)\n",
    "    V_data  = tc.tensor(d_pot  ,dtype=tc.float).reshape(-1,1)\n",
    "\n",
    "    # Criando o modelo\n",
    "\n",
    "    model   = Regressao_M(neuronio = neuronio_,M=camadas,output=1,activation=fun_activation,creat_p = True,N_of_paramater = 5)\n",
    "    # Parâmetros gerais da rede (exclui 'acceleration')\n",
    "    params_network = [p for n, p in model.named_parameters() if 'acceleration' not in n]\n",
    "\n",
    "    # Apenas os parâmetros 'acceleration'\n",
    "    params_acceleration = model.acceleration\n",
    "\n",
    "    # Otimizador com grupos de parâmetros separados\n",
    "    opt = tc.optim.Adam([\n",
    "        {'params': params_network},  # Usa a taxa de aprendizado padrão\n",
    "        {'params': params_acceleration, 'lr': 0.1}  # Taxa de aprendizado específica para os parâmetros 'acceleration'\n",
    "    ], lr=lr_)  # Taxa de aprendizado padrão para os outros parâmetros\n",
    "\n",
    "    #opt     = tc.optim.Adam(params=model.parameters(),lr=lr_,betas=(0.9,0.999))\n",
    "    #lr_step = tc.optim.lr_scheduler.StepLR(opt, step_size=step_size_, gamma=0.9)\n",
    "  \n",
    "    # Configurar o ReduceLROnPlateau\n",
    "    lr_step = tc.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=step_size_)\n",
    "\n",
    "\n",
    "    LOSS    = []\n",
    "    LOSS_edo = []\n",
    "    LOSS_data = []\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        x_preds = model(tc.cat((t_train,I_data,V_data),dim=1))\n",
    "        G0,C_1,C_2,C_3,C_4 = model.acceleration\n",
    "        ###########################################################################\n",
    "        # Derivando\n",
    "        dx_dt = tc.autograd.grad(x_preds,t_train, grad_outputs=tc.ones_like(x_preds), create_graph=True,retain_graph=True)[0]\n",
    "        # Edo\n",
    "        loss_ode_dydt = tc.mean( abs(dx_dt + x_preds -C_1*V_data - C_2*(V_data)**2-C_3*(V_data)**3 - C_4*(V_data)**4)  ) # d\n",
    "        # Usando os dado experimentais\n",
    "        loss_data = tc.mean(abs(I_data/V_data - G0 - x_preds) )\n",
    "        loss_cc = abs(x_preds[0]-x_preds[-1])  \n",
    "        loss    = loss_ode_dydt + loss_data  +loss_cc\n",
    "        ############################################################################\n",
    "        opt.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        opt.step()\n",
    "        lr_step.step(loss)\n",
    "        #lr_step.step()\n",
    "        LOSS.append(loss.detach().numpy())\n",
    "        LOSS_edo.append(loss_ode_dydt.detach().numpy())\n",
    "        LOSS_data.append(loss_data.detach().numpy())\n",
    "        \n",
    "    print(model.acceleration)\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(x_preds.detach().numpy(),\"r--\",label=\"Rede neural\")\n",
    "    plt.plot((I_data/V_data).detach().numpy(),\"k-\",label=\"Data :I(t)/V(t)\")\n",
    "    plt.plot(x_true,\"-\",label=\"RK4\")\n",
    "    plt.legend()\n",
    "    # Primeiro subplot\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(LOSS,\"k\",label=\"loss total\")\n",
    "    plt.plot(LOSS_edo,\"b\",label=\"loss edo\")\n",
    "    plt.plot(LOSS_data,\"r\",label=\"loss data\")\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    G0,C_1,C_2,C_3,C_4 = model.acceleration.detach().numpy()\n",
    "    time,corrent,d_potencial,x_true = numerical_solution2(N,G0,C_1,C_2,C_3,C_4,tau,w,V0)\n",
    "    corrent,d_potencial = corrent/corrent.max(),d_potencial/d_potencial.max()\n",
    "    plt.plot(d_potencial,corrent,\"r-\",label='Predict')\n",
    "    plt.plot(V_data,I_data,\"k--\",label='Data')\n",
    "    plt.ylabel('I(t)')\n",
    "    plt.xlabel('V(t)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print()\n",
    "    print(f\" Predicted : G0 = {G0}, c1 ={C_1}, c2 = {C_2} , c3 ={C_3}, c4 = {C_4} , tau = {tau} , w = {w}, V0 = {V0}\")\n",
    "\n",
    "def treino2(lr_,neuronio_,camadas,fun_activation,epochs=1000,step_size_=500,N=500,G0=1,c1=1.0,c2=1,c3=0,c4=0,tau=1,w=1,V0=1):\n",
    "    # Criando os dados de treino\n",
    "    time,corrent,d_potencial,x_true = numerical_solution2(N,G0,c1,c2,c3,c4,tau,w,V0)\n",
    "    I_n,d_pot = corrent/corrent.max(),d_potencial/d_potencial.max()\n",
    "\n",
    "    t_train = tc.tensor(time   ,requires_grad=True,dtype=tc.float).reshape(-1,1)\n",
    "    I_data  = tc.tensor(I_n    ,dtype=tc.float).reshape(-1,1)\n",
    "    V_data  = tc.tensor(d_pot  ,dtype=tc.float).reshape(-1,1)\n",
    "\n",
    "    # Criando o modelo\n",
    "\n",
    "    model   = Regressao_M(neuronio = neuronio_,M=camadas,output=1,activation=fun_activation)\n",
    "    opt     = tc.optim.Adam(params=model.parameters(),lr=lr_,betas=(0.9,0.999))\n",
    "    lr_step = tc.optim.lr_scheduler.StepLR(opt, step_size=step_size_, gamma=0.9)\n",
    "    LOSS    = []\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        x_preds = model(t_train)\n",
    "        ###########################################################################\n",
    "        # Derivando\n",
    "        dx_dt = tc.autograd.grad(x_preds,t_train, grad_outputs=tc.ones_like(x_preds), create_graph=True,retain_graph=True)[0]\n",
    "        # Edo\n",
    "        loss_ode_dydt = tc.mean( abs(dx_dt + x_preds -c1*V_data - c2*(V_data)**2-c3*(V_data)**3 - c4*(V_data)**4) ) # d\n",
    "        # Usando os dado experimentais\n",
    "        loss_data = tc.mean(abs(I_data/V_data - G0 - x_preds) )\n",
    "        loss_cc = abs(x_preds[0]-x_preds[-1])\n",
    "        \n",
    "        loss = loss_ode_dydt + loss_data  + loss_cc\n",
    "        ############################################################################\n",
    "        LOSS.append(loss.detach().numpy())\n",
    "        opt.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        opt.step()\n",
    "        lr_step.step()\n",
    "    \n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(x_preds.detach().numpy(),\"r--\",label=\"Rede neural\")\n",
    "    plt.plot((I_data/V_data).detach().numpy(),\"k-\",label=\"Data :I(t)/V(t)\")\n",
    "    plt.plot(x_true,\"-\",label=\"RK4\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Primeiro subplot\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(LOSS,label=\"Treino\")\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    time,corrent,d_potencial,x_true = numerical_solution2(N,G0,c1,c2,c3,c4,tau,w,V0)\n",
    "    corrent,d_potencial = corrent/corrent.max(),d_potencial/d_potencial.max()\n",
    "    plt.plot(d_potencial,corrent,\"r-\",label='Predict')\n",
    "    plt.plot(V_data,I_data,\"k--\",label='Data')\n",
    "    plt.ylabel('I(t)')\n",
    "    plt.xlabel('V(t)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print()\n",
    "\n",
    "    print(f\" Data : G0 = {G0}, c1 ={c1}, c2 = {c2} , c3 ={c3}, c4 = {c4} , tau = {tau} , w = {w}, V0 = {V0} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino(lr_=0.0001,neuronio_=5,camadas=2,fun_activation=SineActivation(),epochs=100000,step_size_=5000,G0=1,c1=1.0,c2=1,c3=0,c4=0,tau=1,w=1,V0=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino(lr_=0.001,neuronio_=5,camadas=2,fun_activation=SineActivation(),epochs=100000,step_size_=20000,G0=1,c1=1.0,c2=1,c3=0,c4=0,tau=1,w=1,V0=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
